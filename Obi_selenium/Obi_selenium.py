import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from selenium.webdriver.common.by import By
import pickle
import undetected_chromedriver as uc
import os

# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
cur_data_file = datetime.now().strftime("%m.%Y")
start_time = time.time()


def keep_only_digits_as_int(input_string):
    digits_str = ''.join(filter(str.isdigit, input_string))
    return int(digits_str) if digits_str else 0  # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç, –≤–µ—Ä–Ω—ë—Ç 0


def end_driver(driver):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
    try:
        if driver:
            driver.quit()
            time.sleep(0.5)
    except Exception:
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä "–ù–µ–≤–µ—Ä–Ω—ã–π –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä"
        pass


def load_existing_data(group, city):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞"""
    file_name = f"data_{cur_data_file}_{group}_{city}_obi.json"
    file_path = os.path.join(SCRIPT_DIR, file_name)

    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {file_name}")
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
            return data
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    else:
        print(f"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
        return []


def get_processed_urls(existing_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    processed = set()
    for item in existing_data:
        if '–°—Å—ã–ª–∫–∞' in item and item['–°—Å—ã–ª–∫–∞']:
            processed.add(item['–°—Å—ã–ª–∫–∞'])
    print(f"‚úì –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {len(processed)}")
    return processed


def save_data_incrementally(data_dict, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å—å—é –Ω–∞ –¥–∏—Å–∫"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = file_path + '.tmp'
        with open(temp_file, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä Python
            os.fsync(json_file.fileno())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫ (OS level)

        # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞: –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å —É—Å–ø–µ—à–Ω–∞, –∑–∞–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        if os.path.exists(file_path):
            os.replace(temp_file, file_path)
        else:
            os.rename(temp_file, file_path)

    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass


def save_backup_copy(data_dict, file_path):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ"""
    try:
        backup_path = file_path.replace('.json', '_BACKUP.json')
        temp_backup = backup_path + '.tmp'

        with open(temp_backup, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()
            os.fsync(json_file.fileno())

        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –Ω–æ–≤–æ–π
        if os.path.exists(backup_path):
            os.replace(temp_backup, backup_path)
        else:
            os.rename(temp_backup, backup_path)

        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(data_dict)} –∑–∞–ø–∏—Å–µ–π")

    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        if os.path.exists(temp_backup):
            try:
                os.remove(temp_backup)
            except:
                pass


def save_broken_urls(break_line, group, city):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª"""
    if break_line:
        file_path = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_{group}_{city}_obi.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            for url in break_line:
                file.write(f'{url}\n')


def save_cookies(city_list):
    options = uc.ChromeOptions()
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.images": 2,
        "profile.managed_default_content_settings.media": 2
    }
    options.add_experimental_option("prefs", prefs)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è undetected_chromedriver —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
    driver = uc.Chrome(
        options=options,
        use_subprocess=True,
        version_main=144  # –£–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é Chrome —è–≤–Ω–æ
    )
    time.sleep(5)
    url = 'https://obi.ru/'
    time.sleep(2)

    try:
        for city in city_list:
            xpath_expression = f"//button[@block='StoreItems'][@elem='Item'][text()='{city}']"

            driver.get(url=url)
            time.sleep(2)
            show_password = driver.find_element(By.XPATH, "//button[@class='nbbcX']").click()
            time.sleep(1)
            email_input = driver.find_element(By.XPATH, "//button[@class='_2eEQG _2xhLm _1k0NU _3aV4_']").click()
            time.sleep(1)
            # –í—ã–±–∏—Ä–∞–µ–º –≥–æ—Ä–æ–¥ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏ –∫–ª–∞—Ü–∞–µ–º
            element = driver.find_element(By.XPATH, xpath_expression).click()
            time.sleep(1)
            # –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –ø–µ—á–µ–Ω—å–∫–∏
            cookie_path = os.path.join(SCRIPT_DIR, f'cookies_{city}')
            pickle.dump(driver.get_cookies(), open(cookie_path, 'wb'))
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è cookies –¥–ª—è {city}: {e}")

    finally:
        end_driver(driver)


def get_url_tile(city_list):
    options = uc.ChromeOptions()
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.images": 2,
        "profile.managed_default_content_settings.media": 2
    }
    options.add_experimental_option("prefs", prefs)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è undetected_chromedriver —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
    driver = uc.Chrome(
        options=options,
        use_subprocess=True,
        version_main=144  # –£–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é Chrome —è–≤–Ω–æ
    )
    try:
        for city in city_list:
            url = 'https://obi.ru/'
            driver.get(url=url)

            cookie_path = os.path.join(SCRIPT_DIR, f'cookies_{city}')
            for cookie in pickle.load(open(cookie_path, 'rb')):
                driver.add_cookie(cookie)
            driver.refresh()

            url_group_list = ['https://obi.ru/plitka/plitka-i-keramogranit',
                              'https://obi.ru/santehnika/unitazy-i-instaljacii',
                              'https://obi.ru/santehnika/rakoviny-i-pedestaly']

            for url in url_group_list:
                # –∑–∞—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≥—Ä—É–ø–ø—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
                driver.get(url=url)
                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')

                pages_counts = int(soup.find_all('a', class_='ozZNP')[-1].text)
                url_list = []
                group = url.replace('https://obi.ru/', '').replace('/', '_').replace('-', '_')

                for i in range(1, pages_counts + 1):
                    line = f'{url}?page={i}'

                    driver.get(url=line)
                    content = driver.page_source
                    soup = BeautifulSoup(content, 'lxml')
                    pages = soup.find('div', class_='_2PE29 bm0E6 _1KBT4').find_all('div', class_='FuS7R')

                    for page in pages:
                        try:
                            page_url = 'https://obi.ru' + str(page.find('a').get('href'))
                        except:
                            page_url = None
                        url_list.append(page_url)

                    print(f'–û–±—Ä–∞–±–æ—Ç–∞–ª {i} –∏–∑ {pages_counts} —Å—Ç—Ä–∞–Ω–∏—Ü')

                url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_{group}_{city}_obi.txt')
                with open(url_file_path, 'a', encoding='utf-8') as file:
                    for line in url_list:
                        file.write(f'{line}\n')

    except Exception as ex:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫: {ex}")
    finally:
        end_driver(driver)


def get_data(city_list):
    options = uc.ChromeOptions()
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.images": 2,
        "profile.managed_default_content_settings.media": 2
    }
    options.add_experimental_option("prefs", prefs)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è undetected_chromedriver —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
    driver = uc.Chrome(
        options=options,
        use_subprocess=True,
        version_main=144  # –£–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é Chrome —è–≤–Ω–æ
    )

    try:
        for city in city_list:
            url = 'https://obi.ru/'
            driver.get(url=url)

            cookie_path = os.path.join(SCRIPT_DIR, f'cookies_{city}')
            for cookie in pickle.load(open(cookie_path, 'rb')):
                driver.add_cookie(cookie)
            driver.refresh()

            group_list = ['plitka_plitka_i_keramogranit',
                          'santehnika_unitazy_i_instaljacii',
                          'santehnika_rakoviny_i_pedestaly'
                          ]

            for group in group_list:
                print("\n" + "="*60)
                print(f"–û–ë–†–ê–ë–û–¢–ö–ê: {city} - {group}")
                print("="*60)

                # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                data_dict = load_existing_data(group, city)
                processed_urls = get_processed_urls(data_dict)

                # 2. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
                url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_{group}_{city}_obi.txt')

                if not os.path.exists(url_file_path):
                    print(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {url_file_path}")
                    continue

                with open(url_file_path, 'r', encoding='utf-8') as file:
                    all_lines = [line.strip() for line in file.readlines()]
                    all_lines = list(set(all_lines))  # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

                # 3. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                lines = [line for line in all_lines if line not in processed_urls]

                print(f"–í—Å–µ–≥–æ URL –≤ —Ñ–∞–π–ª–µ: {len(all_lines)}")
                print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_urls)}")
                print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(lines)}")
                print("="*60 + "\n")

                if not lines:
                    print("‚úì –í—Å–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                    continue

                break_line = []
                file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_{group}_{city}_obi.json")
                total_urls = len(lines)
                processed_count = 0

                for idx, line in enumerate(lines, 1):
                    try:
                        print(f"\n[{idx}/{total_urls}] –ó–∞–≥—Ä—É–∑–∫–∞: {line}")
                        driver.get(url=line)

                        try:
                            element = driver.find_element(By.XPATH, "//button[@class='_1np8r']")
                            driver.execute_script("arguments[0].click();", element)
                            time.sleep(1)
                        except:
                            pass

                        try:
                            element = driver.find_element(By.XPATH, "//button[@class='Rl-jS']")
                            driver.execute_script("arguments[0].click();", element)
                            time.sleep(1)
                        except:
                            pass

                        content = driver.page_source
                        soup = BeautifulSoup(content, 'lxml')
                        cur_data = datetime.now().strftime("%d.%m.%Y")
                        cur_time = datetime.now().strftime("%H:%M")

                        try:
                            name = soup.find("h2", class_='_3LdDm').text.strip()
                        except:
                            name = "None"

                        try:
                            price_units = soup.find("span", class_='_3SDdj').text.strip()
                        except:
                            price_units = 'Error'

                        try:
                            new_price = soup.find("span", class_='_3IeOW').text.strip()
                        except:
                            new_price = 'Error'

                        try:
                            sale = soup.find('div', class_='i7rKk').find('div', class_='JpZgV').text.strip()
                        except:
                            sale = None

                        try:
                            stocs = " ".join(soup.find("span", class_="_2KVcZ AX0Hx").text.strip().split())
                        except:
                            stocs = "Error"

                        try:
                            on_sale = soup.find('ul', class_='_1IX-e _1oifM').text.strip()
                        except:
                            on_sale = None

                        left_spec = []
                        right_spec = []

                        specs = soup.find('div', class_='_275gt').find_all('dt')
                        for spec in specs:
                            spec = " ".join(spec.text.strip().split())
                            left_spec.append(spec)

                        rspecs = soup.find('div', class_='_275gt').find_all('dd')
                        for rspec in rspecs:
                            rspec = " ".join(rspec.text.strip().split())
                            right_spec.append(rspec)
                        specs_dict = {left_spec[i].strip(): right_spec[i].strip() for i in range(len(left_spec))}

                        left_stocks = []
                        right_stocks = []
                        stocs_counter = 0

                        # —Å–æ–±–∏—Ä–∞–µ–º —Å–∫–ª–∞–¥—ã
                        try:
                            quant_stock = soup.find_all('div', class_='_2cZg4')
                            for spec in quant_stock:
                                lspec = spec.find("span", class_='_1u7d6').text.strip()
                                left_stocks.append(lspec)
                                rspec = spec.find("span", class_='_2KVcZ AX0Hx').text.strip()
                                right_stocks.append(rspec)

                                try:
                                    stocs_counter += keep_only_digits_as_int(rspec)
                                except:
                                    pass

                            quant_stock_dict = {left_stocks[i].strip(): right_stocks[i].strip() for i in
                                                range(len(right_stocks))}
                        except:
                            quant_stock_dict = {}

                        data = {
                            "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                            "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                            "–†–∞–∑–º–µ—Ä —Å–∫–∏–¥–∫–∏": sale,
                            "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                            "–í –Ω–∞–ª–∏—á–∏–∏": stocs,
                            "–°—Å—ã–ª–∫–∞": line,
                            "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                            "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                            "–ú–∞–≥–∞–∑–∏–Ω": "OBI",
                            "–ì–æ—Ä–æ–¥": city,
                            '–†–∞—Å–ø—Ä–æ–¥–∞–∂–∞': on_sale,
                            "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocs_counter
                        }

                        data_dict.append(data | specs_dict | quant_stock_dict)

                        # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
                        save_data_incrementally(data_dict, file_path)

                        # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                        if len(data_dict) % 1000 == 0:
                            save_backup_copy(data_dict, file_path)

                        print(f'‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{total_urls} | –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')
                        processed_count += 1

                    except Exception as e:
                        break_line.append(line)
                        print(f'‚úó –û—à–∏–±–∫–∞ ({idx}/{total_urls}): {str(e)[:100]}')
                        # –î–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏
                        save_data_incrementally(data_dict, file_path)

                print(f'\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö: {processed_count}')
                print(f'‚úó –û—à–∏–±–æ–∫: {len(break_line)}')
                print(f'‚úì –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')

                # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
                if len(data_dict) > 0:
                    save_backup_copy(data_dict, file_path)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if break_line:
                    save_broken_urls(break_line, group, city)

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
    finally:
        end_driver(driver)


def retry_broken_urls(city_list):
    """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
    options = uc.ChromeOptions()
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.images": 2,
        "profile.managed_default_content_settings.media": 2
    }
    options.add_experimental_option("prefs", prefs)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è undetected_chromedriver —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
    driver = uc.Chrome(
        options=options,
        use_subprocess=True,
        version_main=144  # –£–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é Chrome —è–≤–Ω–æ
    )

    try:
        for city in city_list:
            url = 'https://obi.ru/'
            driver.get(url=url)

            cookie_path = os.path.join(SCRIPT_DIR, f'cookies_{city}')
            for cookie in pickle.load(open(cookie_path, 'rb')):
                driver.add_cookie(cookie)
            driver.refresh()

            group_list = ['plitka_plitka_i_keramogranit',
                          'santehnika_unitazy_i_instaljacii',
                          'santehnika_rakoviny_i_pedestaly'
                          ]

            for group in group_list:
                print("\n" + "="*60)
                print(f"–ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: {city} - {group}")
                print("="*60)

                # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
                data_dict = load_existing_data(group, city)
                processed_urls = get_processed_urls(data_dict)

                # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
                broken_urls_file = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_{group}_{city}_obi.txt')

                if not os.path.exists(broken_urls_file):
                    print(f"‚úì –§–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    continue

                # 3. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL
                with open(broken_urls_file, 'r', encoding='utf-8') as file:
                    all_broken_urls = [line.strip() for line in file.readlines() if line.strip()]

                # 4. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
                lines = [line for line in all_broken_urls if line not in processed_urls]

                print(f"–í—Å–µ–≥–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL: {len(all_broken_urls)}")
                print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ: {len(all_broken_urls) - len(lines)}")
                print(f"–ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(lines)}")
                print("="*60 + "\n")

                if not lines:
                    print("‚úì –í—Å–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
                    continue

                break_line = []
                file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_{group}_{city}_obi.json")
                total_urls = len(lines)
                processed_count = 0

                # 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ª–æ–º–∞–Ω–Ω—ã–π URL
                for idx, line in enumerate(lines, 1):
                    try:
                        print(f"\n[{idx}/{total_urls}] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {line}")
                        driver.get(url=line)
                        time.sleep(3)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Å—ã–ª–æ–∫

                        try:
                            element = driver.find_element(By.XPATH, "//button[@class='_1np8r']")
                            driver.execute_script("arguments[0].click();", element)
                            time.sleep(1)
                        except:
                            pass

                        try:
                            element = driver.find_element(By.XPATH, "//button[@class='Rl-jS']")
                            driver.execute_script("arguments[0].click();", element)
                            time.sleep(1)
                        except:
                            pass

                        content = driver.page_source
                        soup = BeautifulSoup(content, 'lxml')
                        cur_data = datetime.now().strftime("%d.%m.%Y")
                        cur_time = datetime.now().strftime("%H:%M")

                        try:
                            name = soup.find("h2", class_='_3LdDm').text.strip()
                        except:
                            name = "None"

                        if name == "None":
                            print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
                            break_line.append(line)
                            continue

                        try:
                            price_units = soup.find("span", class_='_3SDdj').text.strip()
                        except:
                            price_units = 'Error'

                        try:
                            new_price = soup.find("span", class_='_3IeOW').text.strip()
                        except:
                            new_price = 'Error'

                        try:
                            sale = soup.find('div', class_='i7rKk').find('div', class_='JpZgV').text.strip()
                        except:
                            sale = None

                        try:
                            stocs = " ".join(soup.find("span", class_="_2KVcZ AX0Hx").text.strip().split())
                        except:
                            stocs = "Error"

                        try:
                            on_sale = soup.find('ul', class_='_1IX-e _1oifM').text.strip()
                        except:
                            on_sale = None

                        left_spec = []
                        right_spec = []

                        specs = soup.find('div', class_='_275gt').find_all('dt')
                        for spec in specs:
                            spec = " ".join(spec.text.strip().split())
                            left_spec.append(spec)

                        rspecs = soup.find('div', class_='_275gt').find_all('dd')
                        for rspec in rspecs:
                            rspec = " ".join(rspec.text.strip().split())
                            right_spec.append(rspec)
                        specs_dict = {left_spec[i].strip(): right_spec[i].strip() for i in range(len(left_spec))}

                        left_stocks = []
                        right_stocks = []
                        stocs_counter = 0

                        try:
                            quant_stock = soup.find_all('div', class_='_2cZg4')
                            for spec in quant_stock:
                                lspec = spec.find("span", class_='_1u7d6').text.strip()
                                left_stocks.append(lspec)
                                rspec = spec.find("span", class_='_2KVcZ AX0Hx').text.strip()
                                right_stocks.append(rspec)

                                try:
                                    stocs_counter += keep_only_digits_as_int(rspec)
                                except:
                                    pass

                            quant_stock_dict = {left_stocks[i].strip(): right_stocks[i].strip() for i in
                                                range(len(right_stocks))}
                        except:
                            quant_stock_dict = {}

                        data = {
                            "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                            "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                            "–†–∞–∑–º–µ—Ä —Å–∫–∏–¥–∫–∏": sale,
                            "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                            "–í –Ω–∞–ª–∏—á–∏–∏": stocs,
                            "–°—Å—ã–ª–∫–∞": line,
                            "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                            "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                            "–ú–∞–≥–∞–∑–∏–Ω": "OBI",
                            "–ì–æ—Ä–æ–¥": city,
                            '–†–∞—Å–ø—Ä–æ–¥–∞–∂–∞': on_sale,
                            "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocs_counter
                        }

                        data_dict.append(data | specs_dict | quant_stock_dict)

                        # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò
                        save_data_incrementally(data_dict, file_path)

                        # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                        if len(data_dict) % 1000 == 0:
                            save_backup_copy(data_dict, file_path)

                        print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ [{idx}/{total_urls}]")
                        processed_count += 1

                    except Exception as e:
                        break_line.append(line)
                        print(f'‚úó –û—à–∏–±–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({idx}/{total_urls}): {str(e)[:100]}')
                        save_data_incrementally(data_dict, file_path)

                print(f'\n{"="*60}')
                print(f'‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}')
                print(f'‚úó –í—Å—ë –µ—â—ë —Å–ª–æ–º–∞–Ω–æ: {len(break_line)}')
                print(f'‚úì –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(data_dict)}')
                print("="*60)

                # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
                if len(data_dict) > 0:
                    save_backup_copy(data_dict, file_path)

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
                if break_line:
                    save_broken_urls(break_line, group, city)
                    print(f"\n‚úì –û–±–Ω–æ–≤–ª—ë–Ω —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(break_line)} —à—Ç.")
                else:
                    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –µ—Å–ª–∏ –≤—Å–µ —É—Å–ø–µ—à–Ω–æ
                    try:
                        if os.path.exists(broken_urls_file):
                            os.remove(broken_urls_file)
                            print(f"\n‚úì –í—Å–µ —Å—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –§–∞–π–ª {broken_urls_file} —É–¥–∞–ª—ë–Ω.")
                    except:
                        pass

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
    finally:
        end_driver(driver)


def choice_the_cities():
    city_list = ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–ö–∞–∑–∞–Ω—å', '–í–æ–ª–≥–æ–≥—Ä–∞–¥', '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä']

    def show_cities():
        print("\n–¢–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤:")
        for i, city in enumerate(city_list, 1):
            print(f"{i}. {city}")

    def remove_mode():
        while True:
            show_cities()
            if len(city_list) <= 1:
                break

            try:
                num = int(input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –≥–æ—Ä–æ–¥–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (0 - –æ—Ç–º–µ–Ω–∞): "))
                if num == 0:
                    break
                elif 1 <= num <= len(city_list):
                    removed = city_list.pop(num - 1)
                    print(f"–£–¥–∞–ª—ë–Ω: {removed}")
                else:
                    print("–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä")
            except ValueError:
                print("–û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ")

    def select_single_mode():
        show_cities()
        while len(city_list) > 1:
            try:
                num = int(input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –≥–æ—Ä–æ–¥–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: "))
                if 1 <= num <= len(city_list):
                    city_list[:] = [city_list[num - 1]]  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π
                    print(f"–í—ã–±—Ä–∞–Ω –≥–æ—Ä–æ–¥: {city_list[0]}")
                    break
                else:
                    print("–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä")
            except ValueError:
                print("–û—à–∏–±–∫–∞: –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ")

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
    while True:
        show_cities()
        print("\n1 - –£–¥–∞–ª—è—Ç—å –≥–æ—Ä–æ–¥–∞")
        print("2 - –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≥–æ—Ä–æ–¥")
        print("3 - –ó–∞–≤–µ—Ä—à–∏—Ç—å")

        choice = input("–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-3): ")

        if choice == '1':
            remove_mode()
        elif choice == '2':
            select_single_mode()
        elif choice == '3':
            break
        else:
            print("–û—à–∏–±–∫–∞: –≤—ã–±–µ—Ä–∏—Ç–µ 1, 2 –∏–ª–∏ 3")

    print("\n–ò—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –≥–æ—Ä–æ–¥–æ–≤:")
    for city in city_list:
        print(f"- {city}")
    print()
    return city_list
    print('–ù–∞—á–∏–Ω–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –≥–æ—Ä–æ–¥–∞—Ö –≤ —Å–µ—Ç–∏ OBI')


def main():
    city_list = choice_the_cities()
    coockie_question = input('–í—ã –∂–µ–ª–∞–µ—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å coockie —Ñ–∞–π–ª—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –≥–æ—Ä–æ–¥–∞–º –≤ —Å–ø–∏—Å–∫–µ? ("1" - –î–∞; "0" - –ù–µ—Ç): ')
    if coockie_question == "1":
        save_cookies(city_list)

    # 1. –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞
    get_url_tile(city_list)

    # 2. –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
    get_data(city_list)

    # 3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    retry_question = input('\n–í—ã –∂–µ–ª–∞–µ—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫? ("1" - –î–∞; "0" - –ù–µ—Ç): ')
    if retry_question == "1":
        retry_broken_urls(city_list)


if __name__ == '__main__':
    main()
    finish_time = time.time() - start_time
    print(f"–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ {round(finish_time)} —Å–µ–∫—É–Ω–¥ –Ω–∞ —Ä–∞–±–æ—Ç—É —Å–∫—Ä–∏–ø—Ç–∞")
