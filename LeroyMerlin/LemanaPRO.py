import json
import time
import os
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from datetime import datetime
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException

start_time = time.time()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
cur_data_file = datetime.now().strftime("%m.%Y")

options = uc.ChromeOptions()
prefs = {
    "profile.managed_default_content_settings.images": 2,
    "profile.default_content_setting_values.images": 2,
    "profile.managed_default_content_settings.media": 2
}
options.add_experimental_option("prefs", prefs)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è undetected_chromedriver
driver = uc.Chrome(
    options=options,
    use_subprocess=True,
    version_main=144  # –£–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä—Å–∏—é Chrome —è–≤–Ω–æ
)
time.sleep(5)


def end_driver():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
    try:
        if driver:
            driver.quit()
            time.sleep(0.5)
    except Exception:
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä "–ù–µ–≤–µ—Ä–Ω—ã–π –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä"
        pass


def load_existing_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞"""
    file_name = f"data_{cur_data_file}_Tiles_LemanaPRO.json"
    file_path = os.path.join(SCRIPT_DIR, file_name)

    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {file_name}")
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
            return data
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    else:
        print(f"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
        return []


def get_processed_urls(existing_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    processed = set()
    for item in existing_data:
        if '–°—Å—ã–ª–∫–∞' in item and item['–°—Å—ã–ª–∫–∞']:
            processed.add(item['–°—Å—ã–ª–∫–∞'])
    print(f"‚úì –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {len(processed)}")
    return processed


def save_data_incrementally(data_dict, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å—å—é –Ω–∞ –¥–∏—Å–∫"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = file_path + '.tmp'
        with open(temp_file, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä Python
            os.fsync(json_file.fileno())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫ (OS level)

        # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞: –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å —É—Å–ø–µ—à–Ω–∞, –∑–∞–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        if os.path.exists(file_path):
            os.replace(temp_file, file_path)
        else:
            os.rename(temp_file, file_path)

    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass


def save_backup_copy(data_dict, file_path):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ"""
    try:
        backup_path = file_path.replace('.json', '_BACKUP.json')
        temp_backup = backup_path + '.tmp'

        with open(temp_backup, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()
            os.fsync(json_file.fileno())

        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –Ω–æ–≤–æ–π
        if os.path.exists(backup_path):
            os.replace(temp_backup, backup_path)
        else:
            os.rename(temp_backup, backup_path)

        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(data_dict)} –∑–∞–ø–∏—Å–µ–π")

    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        if os.path.exists(temp_backup):
            try:
                os.remove(temp_backup)
            except:
                pass


def keep_only_digits_as_int(input_string):
    digits_str = ''.join(filter(str.isdigit, input_string))
    return int(digits_str) if digits_str else 0


def safe_find(soup, *args, **kwargs):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏—Å–∫–ª—é—á–µ–Ω–∏–π"""
    try:
        element = soup.find(*args, **kwargs)
        return element.text.strip() if element else None
    except (AttributeError, Exception):
        return None


def retry_click(driver, xpath, max_attempts=3, wait_time=2):
    """–ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –∫–ª–∏–∫–∞ –ø–æ —ç–ª–µ–º–µ–Ω—Ç—É —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º"""
    for attempt in range(max_attempts):
        try:
            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–æ 10 —Å–µ–∫—É–Ω–¥
            element = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            time.sleep(1)
            # –°–∫—Ä–æ–ª–ª–∏–º –∫ —ç–ª–µ–º–µ–Ω—Ç—É
            driver.execute_script("arguments[0].scrollIntoView(true);", element)
            time.sleep(0.5)
            # –ö–ª–∏–∫–∞–µ–º —á–µ—Ä–µ–∑ JavaScript
            driver.execute_script("arguments[0].click();", element)
            time.sleep(1)  # –ñ–¥–µ–º –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞
            return True
        except Exception as e:
            print(f"–ü–æ–ø—ã—Ç–∫–∞ –∫–ª–∏–∫–∞ {attempt + 1}/{max_attempts}: {str(e)[:50]}")
            if attempt == max_attempts - 1:
                raise
            time.sleep(wait_time)
    return False


def parse_price(soup):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ü–µ–Ω—É, —Å–∫–∏–¥–∫—É –∏ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è"""
    new_price = None
    discount = None
    price_units = None

    price_selectors = [
        # –í–∞—Ä–∏–∞–Ω—Ç 1: —Å –±–ª–æ–∫–æ–º —Å–∫–∏–¥–∫–∏
        (
            lambda s: s.find('div', {'data-qa': 'prices_mf-pdp'}).find('div', {'data-testid': 'price-block-price'}).find('span', {'data-testid': 'price'}),
        ),
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –±–µ–∑ –±–ª–æ–∫–∞ —Å–∫–∏–¥–∫–∏
        (
            lambda s: s.find('div', {'data-qa': 'prices_mf-pdp'}).find('span', {'data-testid': 'price'}),
        ),
        # –í–∞—Ä–∏–∞–Ω—Ç 3: —Ü–µ–Ω–∞ –∑–∞ –µ–¥–∏–Ω–∏—Ü—É
        (
            lambda s: s.find('div', {'data-qa': 'prices_mf-pdp'}).find('div', {'data-testid': 'price-block-unitprice'}),
        )
    ]

    for price_selector in price_selectors:
        try:
            price_element = price_selector(soup)
            if price_element:
                new_price = safe_find(price_element, 'span', {'data-testid': 'price-integer'})
                price_units = safe_find(price_element, 'span', {'data-testid': 'price-unit'})
                break
        except (AttributeError, Exception):
            continue

    # –û—Ç–¥–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–∫–∏–¥–∫–∏
    try:
        discount_block = soup.find('div', {'data-testid': 'price-block-discount'})
        if discount_block:
            discount_span = discount_block.find('span', {'data-testid': 'marker-text'})
            if discount_span:
                discount = discount_span.text.strip()
    except (AttributeError, Exception):
        pass

    return new_price, discount, price_units


def process_online_only_product(soup):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ '–¢–æ–ª—å–∫–æ –æ–Ω–ª–∞–π–Ω-–∑–∞–∫–∞–∑' - –ë–ï–ó –∫–ª–∏–∫–∞"""
    print("üåê –¢–æ–≤–∞—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–Ω–ª–∞–π–Ω-–∑–∞–∫–∞–∑–∞")

    stocks_counter = 0
    stocks_mesure = None
    quant_stock_dict = {}  # –ü—É—Å—Ç–æ–π - —Å–∫–ª–∞–¥–æ–≤ –Ω–µ—Ç

    # –ò—â–µ–º "–î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –∑–∞–∫–∞–∑–∞ N –∫–æ—Ä./—à—Ç."
    import re
    try:
        page_text = soup.get_text()
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
        pattern = r'–¥–æ—Å—Ç—É–ø–Ω–æ\s+–¥–ª—è\s+–∑–∞–∫–∞–∑–∞\s+(\d+)\s*(–∫–æ—Ä\.|—à—Ç\.)'
        match = re.search(pattern, page_text.lower())
        if match:
            stocks_counter = int(match.group(1))
            stocks_mesure = match.group(2)
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ: {stocks_counter} {stocks_mesure}")
    except Exception as e:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –æ–Ω–ª–∞–π–Ω-–æ—Å—Ç–∞—Ç–æ–∫: {e}")

    return quant_stock_dict, stocks_counter, stocks_mesure


def process_store_product(driver, soup):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–≤–∞—Ä–∞ –≤ –º–∞–≥–∞–∑–∏–Ω–∞—Ö - –° –∫–ª–∏–∫–æ–º –Ω–∞ —Å–∫–ª–∞–¥—ã"""
    print("üè™ –¢–æ–≤–∞—Ä –≤ –º–∞–≥–∞–∑–∏–Ω–∞—Ö")

    quant_stock_dict = {}
    stocks_counter = 0
    stocks_mesure = None

    try:
        # –ö–ª–∏–∫–∞–µ–º –Ω–∞ –∫–Ω–æ–ø–∫—É —Å–∫–ª–∞–¥–æ–≤
        retry_click(driver, "//*[@data-qa='stock-in-stores-title-interactive']")

        print("–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö —Å–∫–ª–∞–¥–æ–≤...")
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, "m1e45js0_pdp"))
        )
        time.sleep(2)

        # –û–±–Ω–æ–≤–ª—è–µ–º HTML –ø–æ—Å–ª–µ –∫–ª–∏–∫–∞
        content = driver.page_source
        soup = BeautifulSoup(content, 'lxml')

        # –°–æ–±–∏—Ä–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –ø–æ —Å–∫–ª–∞–¥–∞–º
        quant_stock = soup.find_all('div', class_='m1e45js0_pdp')
        for spec in quant_stock:
            store_name = safe_find(spec, "div", class_='m19407om_pdp')
            stock_text = safe_find(spec, "span", {'data-qa': 'modal-store-item-in-stock-text'})

            if store_name and stock_text:
                quant_stock_dict[store_name] = stock_text
                stocks_counter += keep_only_digits_as_int(stock_text)

                if stocks_mesure is None:
                    stocks_mesure = '—à—Ç.' if '—à—Ç.' in stock_text else '–∫–æ—Ä.'

        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(quant_stock_dict)} —Å–∫–ª–∞–¥–æ–≤, –æ—Å—Ç–∞—Ç–æ–∫: {stocks_counter} {stocks_mesure}")

    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–∫–ª–∞–¥–æ–≤: {e}")

    return quant_stock_dict, stocks_counter, stocks_mesure


def get_pages():
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
    groups = [
        "keramogranit",
        "keramicheskaya-plitka",
        "napolnaya-plitka",
        'nastennaya-plitka',
    ]
    url_list = []

    try:
        for group in groups:
            url = f'https://lemanapro.ru/catalogue/{group}/?deliveryType=–°–∞–º–æ–≤—ã–≤–æ–∑+–≤+–º–∞–≥–∞–∑–∏–Ω–µ_–ü—É–Ω–∫—Ç—ã+–≤—ã–¥–∞—á–∏_–î–æ—Å—Ç–∞–≤–∫–∞+–∫—É—Ä—å–µ—Ä–æ–º'
            driver.get(url=url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            time.sleep(5)

            content = driver.page_source
            soup = BeautifulSoup(content, 'lxml')

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
            try:
                pages_count = int(
                    soup.find('nav', class_='V0mKVjE3ab_plp')
                    .find('ul')
                    .find_all('span', class_='JhFg2lLR4e_plp')[-2].text
                )
            except (AttributeError, ValueError, IndexError):
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è {group}")
                continue

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞
            for page_num in range(1, pages_count + 1):
                print(f'–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {page_num} —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞ {group}')
                url = f'https://lemanapro.ru/catalogue/{group}/?deliveryType=–°–∞–º–æ–≤—ã–≤–æ–∑+–≤+–º–∞–≥–∞–∑–∏–Ω–µ_–ü—É–Ω–∫—Ç—ã+–≤—ã–¥–∞—á–∏_–î–æ—Å—Ç–∞–≤–∫–∞+–∫—É—Ä—å–µ—Ä–æ–º&page={page_num}'
                driver.get(url=url)

                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')

                section = soup.find('section', class_='pfgfjrg_plp')
                if not section:
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç–æ–≤–∞—Ä–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                product_links = section.find_all('a', {'data-qa': 'product-name'})
                for link in product_links:
                    href = link.get('href')
                    if href and '/product/' in href:
                        url_list.append('https://lemanapro.ru' + href)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –∏–∑ –∫–∞—Ä—É—Å–µ–ª–∏
                carousel_links = section.find_all('a', class_='wAxCBuwj4T_product-carousel p5y548z_product-carousel p105rlqh_product-carousel')
                for link in carousel_links:
                    href = link.get('href')
                    if href:
                        url_list.append('https://lemanapro.ru' + href)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        url_list = list(set(url_list))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª
        file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_Tiles_LemanaPRO.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            for url in url_list:
                file.write(f'{url}\n')

        print(f"–°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(url_list)}")

    except Exception as ex:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫: {ex}")


def save_broken_urls(break_line):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª"""
    if break_line:
        file_path = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_Tiles_LemanaPRO.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            for url in break_line:
                file.write(f'{url}\n')


def get_data():
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö –∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç —Å–±–æ–µ–≤"""
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    print("\n" + "="*60)
    print("–ó–ê–ì–†–£–ó–ö–ê –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –î–ê–ù–ù–´–•")
    print("="*60)
    data_dict = load_existing_data()
    processed_urls = get_processed_urls(data_dict)

    break_line = []
    file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_Tiles_LemanaPRO.json")

    try:
        # 2. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
        url_file = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_Tiles_LemanaPRO.txt')

        if not os.path.exists(url_file):
            print(f"‚úó –§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º URL –Ω–µ –Ω–∞–π–¥–µ–Ω: {url_file}")
            print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é get_pages() –¥–ª—è —Å–±–æ—Ä–∞ —Å—Å—ã–ª–æ–∫")
            return

        with open(url_file, 'r', encoding='utf-8') as file:
            all_urls = [line.strip() for line in file.readlines()]

        # 3. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        lines = [url for url in all_urls if url not in processed_urls]

        print(f"\n" + "="*60)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("="*60)
        print(f"–í—Å–µ–≥–æ URL –≤ —Ñ–∞–π–ª–µ: {len(all_urls)}")
        print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_urls)}")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(lines)}")
        print("="*60 + "\n")

        if not lines:
            print("‚úì –í—Å–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return

        total_urls = len(lines)
        processed_count = 0

        # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π URL
        for idx, line in enumerate(lines, 1):
            try:
                print(f"\n[{idx}/{total_urls}] –ó–∞–≥—Ä—É–∑–∫–∞: {line}")

                driver.get(url=line)
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)

                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')
                cur_data = datetime.now().strftime("%d.%m.%Y")
                cur_time = datetime.now().strftime("%H:%M")

                # –ö–õ–Æ–ß–ï–í–û–ô –ú–û–ú–ï–ù–¢: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
                online_marker = safe_find(soup, 'span', {'data-qa': 'online-order-only-message-text'})
                is_online_only = online_marker is not None

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                name = safe_find(soup, "h1", {'data-qa': 'product-name'})

                # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–≤–∞—Ä
                if not name:
                    print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞")
                    break_line.append(line)
                    continue

                articul = safe_find(soup, 'span', class_='t12nw7s2_pdp')
                best_price_text = safe_find(soup, "div", {'data-qa': 'productBestPriceNameplate'})
                new_price, discount, price_units = parse_price(soup)

                # –¶–µ–Ω–∞ –∑–∞ –∫–æ—Ä–æ–±–∫—É
                price_box = None
                try:
                    price_box_elem = soup.find('div', class_='u1bdlfxm_pdp').find('div', {'data-testid': 'price-block-unitprice'})
                    if price_box_elem:
                        price_box = safe_find(price_box_elem, 'span', {'data-testid': 'price-integer'})
                except (AttributeError, Exception):
                    pass

                # –ù–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–∞
                stocks = safe_find(soup, "div", class_="out-of-stock-label") or "–í –Ω–∞–ª–∏—á–∏–∏"

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                specs_dict = {}
                specs = soup.find_all('div', {'data-qa': 'characteristics-list-item'})
                for spec in specs:
                    key = safe_find(spec, "div", class_='dsqv1xm_pdp')
                    value = safe_find(spec, "div", class_='v17yx9hk_pdp')
                    if key and value:
                        specs_dict[key] = value

                # –í–´–ë–û–† –°–¢–†–ê–¢–ï–ì–ò–ò: –û–Ω–ª–∞–π–Ω –∏–ª–∏ –ú–∞–≥–∞–∑–∏–Ω
                if is_online_only:
                    quant_stock_dict, stocks_counter, stocks_mesure = process_online_only_product(soup)
                    product_type = "–¢–æ–ª—å–∫–æ –æ–Ω–ª–∞–π–Ω"
                else:
                    quant_stock_dict, stocks_counter, stocks_mesure = process_store_product(driver, soup)
                    product_type = "–í –º–∞–≥–∞–∑–∏–Ω–∞—Ö"

                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                data = {
                    "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                    "–ê—Ä—Ç–∏–∫—É–ª": articul,
                    "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                    "–°–∫–∏–¥–∫–∞": discount,
                    '–¶–µ–Ω–∞ –∑–∞ –∫–æ—Ä–æ–±–∫—É': price_box,
                    "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                    "–°—Å—ã–ª–∫–∞": line,
                    "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                    "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                    "–ú–∞–≥–∞–∑–∏–Ω": "LemanaPRO",
                    "–í –Ω–∞–ª–∏—á–∏–∏": stocks,
                    '–û–Ω–ª–∞–π–Ω –∑–∞–∫–∞–∑': product_type,  # ‚Üê –ó–î–ï–°–¨ –¢–ò–ü –¢–û–í–ê–†–ê
                    '–õ—É—á—à–∞—è —Ü–µ–Ω–∞': best_price_text,
                    "–ï–¥–∏–Ω–∏—Ü–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞ —Å–∫–ª–∞–¥–µ": stocks_mesure,
                    "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocks_counter
                }

                data_dict.append(data | specs_dict | quant_stock_dict)

                # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
                save_data_incrementally(data_dict, file_path)

                # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if len(data_dict) % 1000 == 0:
                    save_backup_copy(data_dict, file_path)

                # –í—ã–≤–æ–¥
                """ print(f'{"="*60}')
                print(f'‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{total_urls} | –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')
                print(f'{"="*60}')
                print(f'–ù–∞–∑–≤–∞–Ω–∏–µ: {name}')
                print(f'–¢–∏–ø: {product_type}')
                print(f'–¶–µ–Ω–∞: {new_price} {price_units}')
                print(f'–û—Å—Ç–∞—Ç–æ–∫: {stocks_counter} {stocks_mesure}')
                print(f'–°–∫–ª–∞–¥–æ–≤: {len(quant_stock_dict)}')
                print(f'{"="*60}\n') """

                processed_count += 1

            except Exception as e:
                break_line.append(line)
                print(f'‚úó –û—à–∏–±–∫–∞ ({idx}/{total_urls}): {str(e)[:100]}')
                # –î–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏
                save_data_incrementally(data_dict, file_path)

        print(f'\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö: {processed_count}')
        print(f'‚úó –û—à–∏–±–æ–∫: {len(break_line)}')
        print(f'‚úì –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–∂–µ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ
        save_data_incrementally(data_dict, file_path)

    finally:
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*60)
        print("–ó–ê–í–ï–†–®–ï–ù–ò–ï –†–ê–ë–û–¢–´")
        print("="*60)

        try:
            if os.path.exists(file_path):
                print(f"‚úì –§–∞–π–ª: {file_path}")
                print(f"‚úì –ó–∞–ø–∏—Å–µ–π: {len(data_dict)}")
                print(f"‚úì –†–∞–∑–º–µ—Ä: {os.path.getsize(file_path)} –±–∞–π—Ç")
            else:
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                save_data_incrementally(data_dict, file_path)
                print(f"‚úì –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ")

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
            if len(data_dict) > 0:
                save_backup_copy(data_dict, file_path)

        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞: {e}")

        # –°–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if break_line:
            try:
                save_broken_urls(break_line)
                print(f"‚úì –°–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(break_line)}")
            except:
                pass

        print("="*60 + "\n")


def retry_broken_urls():
    """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
    print("\n" + "="*60)
    print("–ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –°–õ–û–ú–ê–ù–ù–´–• –°–°–´–õ–û–ö")
    print("="*60)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    data_dict = load_existing_data()
    processed_urls = get_processed_urls(data_dict)

    break_line = []
    file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_Tiles_LemanaPRO.json")
    broken_urls_file = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_Tiles_LemanaPRO.txt')

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
    if not os.path.exists(broken_urls_file):
        print(f"‚úì –§–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {broken_urls_file}")
        print("‚úì –ù–µ—Ç —Å—Å—ã–ª–æ–∫ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    try:
        # 3. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL
        with open(broken_urls_file, 'r', encoding='utf-8') as file:
            all_broken_urls = [line.strip() for line in file.readlines() if line.strip()]

        # 4. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –æ–Ω–∏ –±—ã–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ)
        lines = [url for url in all_broken_urls if url not in processed_urls]

        print(f"\n–í—Å–µ–≥–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL: {len(all_broken_urls)}")
        print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ: {len(all_broken_urls) - len(lines)}")
        print(f"–ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(lines)}")
        print("="*60 + "\n")

        if not lines:
            print("‚úì –í—Å–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return

        total_urls = len(lines)
        processed_count = 0

        # 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ª–æ–º–∞–Ω–Ω—ã–π URL
        for idx, line in enumerate(lines, 1):
            try:
                print(f"\n[{idx}/{total_urls}] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {line}")

                driver.get(url=line)
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(3)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Å—ã–ª–æ–∫

                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')
                cur_data = datetime.now().strftime("%d.%m.%Y")
                cur_time = datetime.now().strftime("%H:%M")

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
                online_marker = safe_find(soup, 'span', {'data-qa': 'online-order-only-message-text'})
                is_online_only = online_marker is not None

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                name = safe_find(soup, "h1", {'data-qa': 'product-name'})

                # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–≤–∞—Ä
                if not name:
                    print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
                    break_line.append(line)
                    continue

                print(f"‚úì –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ: {name[:50]}...")

                articul = safe_find(soup, 'span', class_='t12nw7s2_pdp')
                best_price_text = safe_find(soup, "div", {'data-qa': 'productBestPriceNameplate'})
                new_price, discount, price_units = parse_price(soup)

                # –¶–µ–Ω–∞ –∑–∞ –∫–æ—Ä–æ–±–∫—É
                price_box = None
                try:
                    price_box_elem = soup.find('div', class_='u1bdlfxm_pdp').find('div', {'data-testid': 'price-block-unitprice'})
                    if price_box_elem:
                        price_box = safe_find(price_box_elem, 'span', {'data-testid': 'price-integer'})
                except (AttributeError, Exception):
                    pass

                # –ù–∞–ª–∏—á–∏–µ —Ç–æ–≤–∞—Ä–∞
                stocks = safe_find(soup, "div", class_="out-of-stock-label") or "–í –Ω–∞–ª–∏—á–∏–∏"

                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                specs_dict = {}
                specs = soup.find_all('div', {'data-qa': 'characteristics-list-item'})
                for spec in specs:
                    key = safe_find(spec, "div", class_='dsqv1xm_pdp')
                    value = safe_find(spec, "div", class_='v17yx9hk_pdp')
                    if key and value:
                        specs_dict[key] = value

                # –í–´–ë–û–† –°–¢–†–ê–¢–ï–ì–ò–ò: –û–Ω–ª–∞–π–Ω –∏–ª–∏ –ú–∞–≥–∞–∑–∏–Ω
                if is_online_only:
                    quant_stock_dict, stocks_counter, stocks_mesure = process_online_only_product(soup)
                    product_type = "–¢–æ–ª—å–∫–æ –æ–Ω–ª–∞–π–Ω"
                else:
                    quant_stock_dict, stocks_counter, stocks_mesure = process_store_product(driver, soup)
                    product_type = "–í –º–∞–≥–∞–∑–∏–Ω–∞—Ö"

                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                data = {
                    "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                    "–ê—Ä—Ç–∏–∫—É–ª": articul,
                    "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                    "–°–∫–∏–¥–∫–∞": discount,
                    '–¶–µ–Ω–∞ –∑–∞ –∫–æ—Ä–æ–±–∫—É': price_box,
                    "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                    "–°—Å—ã–ª–∫–∞": line,
                    "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                    "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                    "–ú–∞–≥–∞–∑–∏–Ω": "LemanaPRO",
                    "–í –Ω–∞–ª–∏—á–∏–∏": stocks,
                    '–û–Ω–ª–∞–π–Ω –∑–∞–∫–∞–∑': product_type,
                    '–õ—É—á—à–∞—è —Ü–µ–Ω–∞': best_price_text,
                    "–ï–¥–∏–Ω–∏—Ü–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞ —Å–∫–ª–∞–¥–µ": stocks_mesure,
                    "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocks_counter
                }

                data_dict.append(data | specs_dict | quant_stock_dict)

                # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò
                save_data_incrementally(data_dict, file_path)

                # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if len(data_dict) % 1000 == 0:
                    save_backup_copy(data_dict, file_path)

                processed_count += 1

                print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ [{idx}/{total_urls}]")

            except Exception as e:
                break_line.append(line)
                print(f'‚úó –û—à–∏–±–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({idx}/{total_urls}): {str(e)[:100]}')
                save_data_incrementally(data_dict, file_path)

        print(f'\n{"="*60}')
        print(f'‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}')
        print(f'‚úó –í—Å—ë –µ—â—ë —Å–ª–æ–º–∞–Ω–æ: {len(break_line)}')
        print(f'‚úì –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(data_dict)}')
        print("="*60)

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
        save_data_incrementally(data_dict, file_path)

    finally:
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
        try:
            if len(data_dict) > 0:
                save_backup_copy(data_dict, file_path)
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        if break_line:
            save_broken_urls(break_line)
            print(f"\n‚úì –û–±–Ω–æ–≤–ª—ë–Ω —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(break_line)} —à—Ç.")
        else:
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –µ—Å–ª–∏ –≤—Å–µ —É—Å–ø–µ—à–Ω–æ
            try:
                if os.path.exists(broken_urls_file):
                    os.remove(broken_urls_file)
                    print(f"\n‚úì –í—Å–µ —Å—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –§–∞–π–ª {broken_urls_file} —É–¥–∞–ª—ë–Ω.")
            except Exception:
                pass

        print("\n")


def main():
    try:
        # 1. –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        # get_pages()

        # 2. –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
        get_data()

        # 3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        retry_broken_urls()

    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –ü–û–°–õ–ï –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        print("\n" + "="*60)
        print("–ó–ê–ö–†–´–¢–ò–ï –ë–†–ê–£–ó–ï–†–ê")
        print("="*60)
        end_driver()
        print("‚úì –ë—Ä–∞—É–∑–µ—Ä –∑–∞–∫—Ä—ã—Ç")
        print("="*60 + "\n")


if __name__ == '__main__':
    main()
    finish_time = time.time() - start_time
    print(f"–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–±–æ—Ç—É —Å–∫—Ä–∏–ø—Ç–∞ –≤—Ä–µ–º—è: {finish_time}")