import json
import time
import os
import pickle
import undetected_chromedriver as uc
from bs4 import BeautifulSoup
from datetime import datetime
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException

start_time = time.time()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
COOKIES_FILE = os.path.join(SCRIPT_DIR, 'petrovich_cookies.pkl')
cur_data_file = datetime.now().strftime("%m.%Y")


def keep_only_digits_as_int(input_string):
    digits_str = ''.join(filter(str.isdigit, input_string))
    return int(digits_str) if digits_str else 0  # –ï—Å–ª–∏ —Ü–∏—Ñ—Ä –Ω–µ—Ç, –≤–µ—Ä–Ω—ë—Ç 0


def end_driver(driver):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –¥—Ä–∞–π–≤–µ—Ä–∞ –±—Ä–∞—É–∑–µ—Ä–∞"""
    try:
        if driver:
            driver.quit()
            time.sleep(0.5)
    except Exception:
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –∑–∞–∫—Ä—ã—Ç–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä "–ù–µ–≤–µ—Ä–Ω—ã–π –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä"
        pass


def save_cookies(driver):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç cookies –±—Ä–∞—É–∑–µ—Ä–∞ –≤ —Ñ–∞–π–ª"""
    with open(COOKIES_FILE, 'wb') as f:
        pickle.dump(driver.get_cookies(), f)
    print("[OK] Cookies —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ petrovich_cookies.pkl")


def load_cookies(driver):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç cookies –∏–∑ —Ñ–∞–π–ª–∞ –≤ –±—Ä–∞—É–∑–µ—Ä"""
    if not os.path.exists(COOKIES_FILE):
        return False
    try:
        with open(COOKIES_FILE, 'rb') as f:
            cookies = pickle.load(f)
        for cookie in cookies:
            # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –≤—ã–∑–≤–∞—Ç—å –æ—à–∏–±–∫—É
            cookie.pop('sameSite', None)
            cookie.pop('storeId', None)
            try:
                driver.add_cookie(cookie)
            except Exception:
                pass
        print("[OK] Cookies –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞")
        return True
    except Exception as e:
        print(f"[!] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ cookies: {e}")
        return False


def _create_driver(block_images=True):
    """–°–æ–∑–¥–∞–µ—Ç –¥—Ä–∞–π–≤–µ—Ä —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    options = uc.ChromeOptions()
    if block_images:
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.images": 2,
            "profile.managed_default_content_settings.media": 2
        }
        options.add_experimental_option("prefs", prefs)

    driver = uc.Chrome(
        options=options,
        use_subprocess=True,
        version_main=144
    )
    return driver


def init_driver_with_cookies():
    """–°–æ–∑–¥–∞–µ—Ç –¥—Ä–∞–π–≤–µ—Ä –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç cookies, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ—Å–∏—Ç —Ä–µ—à–∏—Ç—å –∫–∞–ø—á—É"""

    # 1. –ü—Ä–æ–±—É–µ–º –±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å: –¥—Ä–∞–π–≤–µ—Ä —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∫–∞—Ä—Ç–∏–Ω–æ–∫ + cookies
    if os.path.exists(COOKIES_FILE):
        driver = _create_driver(block_images=True)
        driver.get("https://petrovich.ru")
        time.sleep(2)
        load_cookies(driver)
        driver.get("https://petrovich.ru")
        time.sleep(2)

        if not _is_captcha_present(driver):
            print("[OK] Cookies —Ä–∞–±–æ—Ç–∞—é—Ç, –∫–∞–ø—á–∏ –Ω–µ—Ç")
            return driver

        # Cookies –Ω–µ –ø–æ–º–æ–≥–ª–∏ -- –∑–∞–∫—Ä—ã–≤–∞–µ–º —ç—Ç–æ—Ç –¥—Ä–∞–π–≤–µ—Ä
        print("[!] Cookies —É—Å—Ç–∞—Ä–µ–ª–∏, –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å –∫–∞–ø—á—É –∑–∞–Ω–æ–≤–æ")
        end_driver(driver)

    # 2. –û—Ç–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –° –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∫–∞–ø—á–∏
    print("[...] –û—Ç–∫—Ä—ã–≤–∞—é –±—Ä–∞—É–∑–µ—Ä —Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∫–∞–ø—á–∏...")
    driver = _create_driver(block_images=False)
    driver.get("https://petrovich.ru")
    time.sleep(2)

    if _is_captcha_present(driver):
        _wait_for_manual_captcha(driver)
    save_cookies(driver)
    end_driver(driver)

    # 3. –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á–∏–π –¥—Ä–∞–π–≤–µ—Ä –ë–ï–ó –∫–∞—Ä—Ç–∏–Ω–æ–∫ + —Å–≤–µ–∂–∏–µ cookies
    driver = _create_driver(block_images=True)
    driver.get("https://petrovich.ru")
    time.sleep(2)
    load_cookies(driver)
    driver.get("https://petrovich.ru")
    time.sleep(2)
    print("[OK] –†–∞–±–æ—á–∏–π –¥—Ä–∞–π–≤–µ—Ä –≥–æ—Ç–æ–≤")

    return driver


def _is_captcha_present(driver):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –∫–∞–ø—á–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
    page_source = driver.page_source.lower()
    captcha_signs = ['captcha', 'recaptcha', 'challenge', 'smartcaptcha', 'checkbox-captcha']
    return any(sign in page_source for sign in captcha_signs)


def _wait_for_manual_captcha(driver):
    """–û–∂–∏–¥–∞–µ—Ç —Ä—É—á–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è –∫–∞–ø—á–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
    print("\n" + "=" * 60)
    print("–û–ë–ù–ê–†–£–ñ–ï–ù–ê –ö–ê–ü–ß–ê!")
    print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Ä–µ—à–∏—Ç–µ –∫–∞–ø—á—É –≤ –æ—Ç–∫—Ä—ã—Ç–æ–º –±—Ä–∞—É–∑–µ—Ä–µ.")
    print("–ü–æ—Å–ª–µ —Ä–µ—à–µ–Ω–∏—è –Ω–∞–∂–º–∏—Ç–µ Enter –∑–¥–µ—Å—å...")
    print("=" * 60)
    input("> ")
    time.sleep(2)
    print("[OK] –ö–∞–ø—á–∞ —Ä–µ—à–µ–Ω–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É")


def load_existing_data(group):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞"""
    file_name = f"data_{cur_data_file}_{group}_Petrovich.json"
    file_path = os.path.join(SCRIPT_DIR, file_name)

    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {file_name}")
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
            return data
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    else:
        print(f"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
        return []


def get_processed_urls(existing_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    processed = set()
    for item in existing_data:
        if '–°—Å—ã–ª–∫–∞' in item and item['–°—Å—ã–ª–∫–∞']:
            processed.add(item['–°—Å—ã–ª–∫–∞'])
    print(f"‚úì –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {len(processed)}")
    return processed


def save_data_incrementally(data_dict, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å—å—é –Ω–∞ –¥–∏—Å–∫"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = file_path + '.tmp'
        with open(temp_file, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä Python
            os.fsync(json_file.fileno())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫ (OS level)

        # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞: –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å —É—Å–ø–µ—à–Ω–∞, –∑–∞–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        if os.path.exists(file_path):
            os.replace(temp_file, file_path)
        else:
            os.rename(temp_file, file_path)

    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass


def save_backup_copy(data_dict, file_path):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ"""
    try:
        backup_path = file_path.replace('.json', '_BACKUP.json')
        temp_backup = backup_path + '.tmp'

        with open(temp_backup, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()
            os.fsync(json_file.fileno())

        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –Ω–æ–≤–æ–π
        if os.path.exists(backup_path):
            os.replace(temp_backup, backup_path)
        else:
            os.rename(temp_backup, backup_path)

        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(data_dict)} –∑–∞–ø–∏—Å–µ–π")

    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        if os.path.exists(temp_backup):
            try:
                os.remove(temp_backup)
            except:
                pass


def save_broken_urls(break_line, group):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª"""
    if break_line:
        file_path = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_{group}_Petrovich.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            for url in break_line:
                file.write(f'{url}\n')


def choice_group():
    available_groups = [
        "1351/?material=glazurovannyi_keramogranit|keramika|keramicheskaya_plitka|klinker|tehnicheskii_keramogranit",
        # –ü–ª–∏—Ç–∫–∞
        "226931838",  # –†–∞–∫–æ–≤–∏–Ω—ã —Å —Ç—É–º–±–æ–π
        "7172",  # –ò–Ω—Å—Ç–∞–ª–ª—è—Ü–∏–∏ —Å —Ç—É–º–±–æ–π
        "177625593",  # –£–Ω–∏—Ç–∞–∑—ã
        "245811690"  # —É–º—ã–≤–∞–ª—å–Ω–∏–∫–∏
    ]

    legends_group = [
        "–ü–ª–∏—Ç–∫–∞",
        "–†–∞–∫–æ–≤–∏–Ω—ã —Å —Ç—É–º–±–æ–π",
        "–ò–Ω—Å—Ç–∞–ª–ª—è—Ü–∏–∏ –¥–ª—è —É–Ω–∏—Ç–∞–∑–æ–≤",
        "–£–Ω–∏—Ç–∞–∑—ã",
        "–£–º—ã–≤–∞–ª—å–Ω–∏–∫–∏"
    ]

    # –°–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –≥—Ä—É–ø–ø

    selected_groups = []

    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ –≥—Ä—É–ø–ø—ã –¥–ª—è –≤—ã–±–æ—Ä–∞:")
    for i, group in enumerate(legends_group, 1):
        print(f"{i}. {group}")

    print("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä–∞ –≥—Ä—É–ø–ø, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏—Ç–µ –≤–∫–ª—é—á–∏—Ç—å (—á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª):")
    user_input = input("> ")

    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–≤–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Å–ø–∏—Å–æ–∫ –Ω–æ–º–µ—Ä–æ–≤
        chosen_indices = list(map(int, user_input.split()))

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –≤ —Å–ø–∏—Å–æ–∫
        for index in chosen_indices:
            if 1 <= index <= len(available_groups):
                selected_groups.append(available_groups[index - 1])
            else:
                print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–æ–º–µ—Ä {index} –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º –∏ –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω")

        print("\n–í—ã–±—Ä–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã:")
        for group in selected_groups:
            print(f"- {group}")
        print("–ù–µ —Å–º–æ—Ç—Ä–∏ –Ω–∞ —ç—Ç–∏ —Ü–∏—Ñ—Ä—ã, —ç—Ç–æ –Ω—É–∂–Ω–æ –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–º —É—Ä–æ–≤–Ω–µ:)")
        print("–ù–∞—á–∏–Ω–∞—é —Ä–∞–±–æ—Ç–∞—Ç—å...\n")

    except ValueError:
        print("–û—à–∏–±–∫–∞: –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–æ–¥–∏—Ç–µ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–∞, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª–∞–º–∏")
    return selected_groups


def get_pages(group):
    driver = init_driver_with_cookies()

    try:

        url = f'https://petrovich.ru/catalog/{group}/'
        driver.get(url=url)
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        time.sleep(1)
        content = driver.page_source
        soup = BeautifulSoup(content, 'lxml')

        try:
            pages_count = int(keep_only_digits_as_int(soup.find('p', {'data-test': "products-counter"}).text)) / 20
            pages_count = int(pages_count) + (pages_count > int(pages_count))
            # print(pages_count)
        except:
            pages_count = 1

        url_list = []
        for i in range(pages_count):
            print(f'–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {i} —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞—Ç–∞–ª–æ–≥–∞ {group}')
            url = f'https://petrovich.ru/catalog/{group}/?sort=popularity_desc&p={i}'
            driver.get(url=url)
            content = driver.page_source
            soup = BeautifulSoup(content, 'lxml')
            pages = soup.find_all('a', {'data-test': "product-link"})

            for page in pages:
                url = page.get('href')
                url_list.append('https://petrovich.ru' + url + '#properties')

        url_list = list(set(url_list))

        if group == '1351/?material=glazurovannyi_keramogranit|keramika|keramicheskaya_plitka|klinker|tehnicheskii_keramogranit':
            group = 'plitka'
        elif group == '226931838':
            group = 'rakovinyandtumby'
        elif group == '7172':
            group = 'instaliyatsiforunitazy'
        elif group == '177625593':
            group = 'unitay'
        else:
            group = 'umivalniki'

        url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_{group}_Petrovich.txt')
        with open(url_file_path, 'a', encoding='utf-8') as file:
            for line in url_list:
                file.write(f'{line}\n')

    except Exception as ex:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ —Å—Å—ã–ª–æ–∫: {ex}")
    finally:
        end_driver(driver)


def get_data(group):
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≥—Ä—É–ø–ø—ã
    if group == '1351/?material=glazurovannyi_keramogranit|keramika|keramicheskaya_plitka|klinker|tehnicheskii_keramogranit':
        group = 'plitka'
    elif group == '226931838':
        group = 'rakovinyandtumby'
    elif group == '7172':
        group = 'instaliyatsiforunitazy'
    elif group == '177625593':
        group = 'unitay'
    else:
        group = 'umivalniki'

    driver = init_driver_with_cookies()

    try:
        print("\n" + "="*60)
        print(f"–û–ë–†–ê–ë–û–¢–ö–ê: {group}")
        print("="*60)

        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        data_dict = load_existing_data(group)
        processed_urls = get_processed_urls(data_dict)

        # 2. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
        url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_{group}_Petrovich.txt')

        if not os.path.exists(url_file_path):
            print(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {url_file_path}")
            return

        with open(url_file_path, 'r', encoding='utf-8') as file:
            all_lines = [line.strip() for line in file.readlines()]

        # 3. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        lines = [line for line in all_lines if line not in processed_urls]

        print(f"–í—Å–µ–≥–æ URL –≤ —Ñ–∞–π–ª–µ: {len(all_lines)}")
        print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_urls)}")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(lines)}")
        print("="*60 + "\n")

        if not lines:
            print("‚úì –í—Å–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return

        break_line = []
        file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_{group}_Petrovich.json")
        total_urls = len(lines)
        processed_count = 0

        for idx, line in enumerate(lines, 1):
            try:
                print(f"\n[{idx}/{total_urls}] –ó–∞–≥—Ä—É–∑–∫–∞: {line}")
                driver.get(url=line)
                time.sleep(0.5)

                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')
                cur_data = datetime.now().strftime("%d.%m.%Y")
                cur_time = datetime.now().strftime("%H:%M")

                try:
                    name = soup.find("h1").text.strip()
                except:
                    name = None

                try:
                    price_units = soup.find('span',{'data-test':'alt-unit-tab'}).text.strip()
                except:
                    try:
                        price_units = soup.find('p',{'data-test':'default-unit-tab'}).text.strip()
                    except:
                        price_units = None

                try:
                    new_price = soup.find('div', class_='sale-block').find('p').text.strip()
                    old_price = soup.find('div', class_='sale-block').find('div',
                                                                           class_='sale-block-previous').text.strip()
                except:
                    new_price = soup.find('div', {'data-test': 'price-block'}).find('p', {
                        'data-test': 'product-gold-price'}).text.strip()
                    old_price = None

                try:
                    price_box = soup.find('div', class_='units-hint').find('span',
                                                                           class_='pt-nowrap tooltip').text.strip()
                except:
                    price_box = None

                left_spec = []
                right_spec = []

                specs = soup.find('ul', class_='product-properties-list listing-data').find_all('li', class_ = 'data-item')
                for spec in specs:
                    lspec = spec.find("div", class_='title').text.strip()
                    left_spec.append(lspec)
                    rspec = spec.find("div", class_='value').text.strip()
                    right_spec.append(rspec)

                specs_dict = {left_spec[i].strip(): right_spec[i].strip() for i in range(len(left_spec))}

                # —Å–æ–±–∏—Ä–∞–µ–º —Å–∫–ª–∞–¥—ã
                stocks_counter = 0
                try:
                    quant_stock = soup.find('div', class_='product-sidebar-content m-desktop').find('span', class_='pt-split-sm-xs-s pt-y-center').find('p', {'data-test':'typography'}).text.strip()

                    try:
                        stocks_counter += keep_only_digits_as_int(quant_stock)
                    except:
                        pass

                except:
                    pass

                data = {
                    "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                    "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                    "–¶–µ–Ω–∞ –±–µ–∑ —Å–∫–∏–¥–∫–∏": old_price,
                    '–ü—Ä–æ–¥–∞–µ—Ç—Å—è –∫–æ—Ä–æ–±–∫–∞–º–∏ –ø–æ': price_box,
                    "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                    "–°—Å—ã–ª–∫–∞": line,
                    "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                    "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                    "–ú–∞–≥–∞–∑–∏–Ω": "Petrovich",
                    "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocks_counter
                }

                data_dict.append(data | specs_dict)

                # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–±–æ–µ–≤)
                save_data_incrementally(data_dict, file_path)

                # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if len(data_dict) % 1000 == 0:
                    save_backup_copy(data_dict, file_path)

                print(f'‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{total_urls} | –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')
                processed_count += 1

            except Exception as e:
                break_line.append(line)
                print(f'‚úó –û—à–∏–±–∫–∞ ({idx}/{total_urls}): {str(e)[:100]}')
                # –î–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ, —á—Ç–æ —É—Å–ø–µ–ª–∏
                save_data_incrementally(data_dict, file_path)

        print(f'\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö: {processed_count}')
        print(f'‚úó –û—à–∏–±–æ–∫: {len(break_line)}')
        print(f'‚úì –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
        if len(data_dict) > 0:
            save_backup_copy(data_dict, file_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        if break_line:
            save_broken_urls(break_line, group)

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–∂–µ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ
        if 'file_path' in locals() and 'data_dict' in locals():
            save_data_incrementally(data_dict, file_path)
    finally:
        end_driver(driver)


def retry_broken_urls(group):
    """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≥—Ä—É–ø–ø—ã
    if group == '1351/?material=glazurovannyi_keramogranit|keramika|keramicheskaya_plitka|klinker|tehnicheskii_keramogranit':
        group = 'plitka'
    elif group == '226931838':
        group = 'rakovinyandtumby'
    elif group == '7172':
        group = 'instaliyatsiforunitazy'
    elif group == '177625593':
        group = 'unitay'
    else:
        group = 'umivalniki'

    driver = init_driver_with_cookies()

    try:
        print("\n" + "="*60)
        print(f"–ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: {group}")
        print("="*60)

        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        data_dict = load_existing_data(group)
        processed_urls = get_processed_urls(data_dict)

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
        broken_urls_file = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_{group}_Petrovich.txt')

        if not os.path.exists(broken_urls_file):
            print(f"‚úì –§–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        # 3. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL
        with open(broken_urls_file, 'r', encoding='utf-8') as file:
            all_broken_urls = [line.strip() for line in file.readlines() if line.strip()]

        # 4. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        lines = [line for line in all_broken_urls if line not in processed_urls]

        print(f"–í—Å–µ–≥–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL: {len(all_broken_urls)}")
        print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ: {len(all_broken_urls) - len(lines)}")
        print(f"–ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(lines)}")
        print("="*60 + "\n")

        if not lines:
            print("‚úì –í—Å–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            return

        break_line = []
        file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_{group}_Petrovich.json")
        total_urls = len(lines)
        processed_count = 0

        # 5. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ª–æ–º–∞–Ω–Ω—ã–π URL
        for idx, line in enumerate(lines, 1):
            try:
                print(f"\n[{idx}/{total_urls}] –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {line}")
                driver.get(url=line)
                time.sleep(3)  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Å—ã–ª–æ–∫

                content = driver.page_source
                soup = BeautifulSoup(content, 'lxml')
                cur_data = datetime.now().strftime("%d.%m.%Y")
                cur_time = datetime.now().strftime("%H:%M")

                try:
                    name = soup.find("h1").text.strip()
                except:
                    name = None

                if not name:
                    print(f"‚ö† –ü—Ä–æ–ø—É—Å–∫: –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ-–ø—Ä–µ–∂–Ω–µ–º—É –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
                    break_line.append(line)
                    continue

                try:
                    price_units = soup.find('span',{'data-test':'alt-unit-tab'}).text.strip()
                except:
                    try:
                        price_units = soup.find('p',{'data-test':'default-unit-tab'}).text.strip()
                    except:
                        price_units = None

                try:
                    new_price = soup.find('div', class_='sale-block').find('p').text.strip()
                    old_price = soup.find('div', class_='sale-block').find('div',
                                                                           class_='sale-block-previous').text.strip()
                except:
                    new_price = soup.find('div', {'data-test': 'price-block'}).find('p', {
                        'data-test': 'product-gold-price'}).text.strip()
                    old_price = None

                try:
                    price_box = soup.find('div', class_='units-hint').find('span',
                                                                           class_='pt-nowrap tooltip').text.strip()
                except:
                    price_box = None

                left_spec = []
                right_spec = []

                specs = soup.find('ul', class_='product-properties-list listing-data').find_all('li', class_ = 'data-item')
                for spec in specs:
                    lspec = spec.find("div", class_='title').text.strip()
                    left_spec.append(lspec)
                    rspec = spec.find("div", class_='value').text.strip()
                    right_spec.append(rspec)

                specs_dict = {left_spec[i].strip(): right_spec[i].strip() for i in range(len(left_spec))}

                # —Å–æ–±–∏—Ä–∞–µ–º —Å–∫–ª–∞–¥—ã
                stocks_counter = 0
                try:
                    quant_stock = soup.find('div', class_='product-sidebar-content m-desktop').find('span', class_='pt-split-sm-xs-s pt-y-center').find('p', {'data-test':'typography'}).text.strip()

                    try:
                        stocks_counter += keep_only_digits_as_int(quant_stock)
                    except:
                        pass

                except:
                    pass

                data = {
                    "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
                    "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
                    "–¶–µ–Ω–∞ –±–µ–∑ —Å–∫–∏–¥–∫–∏": old_price,
                    '–ü—Ä–æ–¥–∞–µ—Ç—Å—è –∫–æ—Ä–æ–±–∫–∞–º–∏ –ø–æ': price_box,
                    "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
                    "–°—Å—ã–ª–∫–∞": line,
                    "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
                    "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
                    "–ú–∞–≥–∞–∑–∏–Ω": "Petrovich",
                    "–û–±—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫": stocks_counter
                }

                data_dict.append(data | specs_dict)

                # –°–û–•–†–ê–ù–ï–ù–ò–ï –ü–û–°–õ–ï –ö–ê–ñ–î–û–ô –ö–ê–†–¢–û–ß–ö–ò
                save_data_incrementally(data_dict, file_path)

                # –†–ï–ó–ï–†–í–ù–û–ï –ö–û–ü–ò–†–û–í–ê–ù–ò–ï –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
                if len(data_dict) % 1000 == 0:
                    save_backup_copy(data_dict, file_path)

                print(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ [{idx}/{total_urls}]")
                processed_count += 1

            except Exception as e:
                break_line.append(line)
                print(f'‚úó –û—à–∏–±–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({idx}/{total_urls}): {str(e)[:100]}')
                save_data_incrementally(data_dict, file_path)

        print(f'\n{"="*60}')
        print(f'‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}')
        print(f'‚úó –í—Å—ë –µ—â—ë —Å–ª–æ–º–∞–Ω–æ: {len(break_line)}')
        print(f'‚úì –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(data_dict)}')
        print("="*60)

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
        if len(data_dict) > 0:
            save_backup_copy(data_dict, file_path)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        if break_line:
            save_broken_urls(break_line, group)
            print(f"\n‚úì –û–±–Ω–æ–≤–ª—ë–Ω —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(break_line)} —à—Ç.")
        else:
            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –µ—Å–ª–∏ –≤—Å–µ —É—Å–ø–µ—à–Ω–æ
            try:
                if os.path.exists(broken_urls_file):
                    os.remove(broken_urls_file)
                    print(f"\n‚úì –í—Å–µ —Å—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –§–∞–π–ª —É–¥–∞–ª—ë–Ω.")
            except:
                pass

    except Exception as ex:
        print(f"‚úó –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {ex}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–∂–µ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–µ
        if 'file_path' in locals() and 'data_dict' in locals():
            save_data_incrementally(data_dict, file_path)
    finally:
        end_driver(driver)


def main():
    selected_groups = choice_group()

    # 1. –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    for group in selected_groups:
        get_pages(group)

    # 2. –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
    for group in selected_groups:
        get_data(group)

    # 3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    retry_question = input('\n–í—ã –∂–µ–ª–∞–µ—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫? ("1" - –î–∞; "0" - –ù–µ—Ç): ')
    if retry_question == "1":
        for group in selected_groups:
            retry_broken_urls(group)


if __name__ == '__main__':
    main()
    finish_time = time.time() - start_time
    print(f"–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–±–æ—Ç—É —Å–∫—Ä–∏–ø—Ç–∞ –≤—Ä–µ–º—è: {finish_time}")
