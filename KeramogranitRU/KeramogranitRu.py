import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
import os
import asyncio
import aiohttp

start_time = time.time()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
cur_data_file = datetime.now().strftime("%m.%Y")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
CONCURRENT_REQUESTS = 6  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_TIMEOUT = 30  # –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
}


def load_existing_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞ —Ç–µ–∫—É—â–µ–≥–æ –º–µ—Å—è—Ü–∞"""
    file_name = f"data_{cur_data_file}_KeramogranitRu.json"
    file_path = os.path.join(SCRIPT_DIR, file_name)

    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª: {file_name}")
            print(f"‚úì –ù–∞–π–¥–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(data)}")
            return data
        except Exception as e:
            print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    else:
        print(f"–§–∞–π–ª {file_name} –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
        return []


def get_processed_urls(existing_data):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö URL –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    processed = set()
    for item in existing_data:
        if '–°—Å—ã–ª–∫–∞' in item and item['–°—Å—ã–ª–∫–∞']:
            processed.add(item['–°—Å—ã–ª–∫–∞'])
    print(f"‚úì –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ URL: {len(processed)}")
    return processed


def save_data_incrementally(data_dict, file_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–ø–∏—Å—å—é –Ω–∞ –¥–∏—Å–∫"""
    try:
        # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_file = file_path + '.tmp'
        with open(temp_file, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä Python
            os.fsync(json_file.fileno())  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–∞ –¥–∏—Å–∫ (OS level)

        # –ê—Ç–æ–º–∞—Ä–Ω–∞—è –∑–∞–º–µ–Ω–∞: –µ—Å–ª–∏ –∑–∞–ø–∏—Å—å —É—Å–ø–µ—à–Ω–∞, –∑–∞–º–µ–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª
        if os.path.exists(file_path):
            os.replace(temp_file, file_path)
        else:
            os.rename(temp_file, file_path)

    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        if os.path.exists(temp_file):
            try:
                os.remove(temp_file)
            except:
                pass


def save_backup_copy(data_dict, file_path):
    """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é, –∫–æ—Ç–æ—Ä–∞—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ"""
    try:
        backup_path = file_path.replace('.json', '_BACKUP.json')
        temp_backup = backup_path + '.tmp'

        with open(temp_backup, 'w', encoding="utf-8") as json_file:
            json.dump(data_dict, json_file, indent=4, ensure_ascii=False)
            json_file.flush()
            os.fsync(json_file.fileno())

        # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –Ω–æ–≤–æ–π
        if os.path.exists(backup_path):
            os.replace(temp_backup, backup_path)
        else:
            os.rename(temp_backup, backup_path)

        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {len(data_dict)} –∑–∞–ø–∏—Å–µ–π")

    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        if os.path.exists(temp_backup):
            try:
                os.remove(temp_backup)
            except:
                pass


def save_broken_urls(break_line):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ —Ñ–∞–π–ª"""
    if break_line:
        file_path = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_KeramogranitRu.txt')
        with open(file_path, 'w', encoding='utf-8') as file:
            for url in break_line:
                file.write(f'{url}\n')


async def fetch_page_async(session, url, page_num=None, total_pages=None):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as response:
            if page_num:
                print(f'–û–±—Ä–∞–±–æ—Ç–∞–ª {page_num} –∏–∑ {total_pages} —Å—Ç—Ä–∞–Ω–∏—Ü')
            return await response.text()
    except Exception as e:
        print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)[:50]}")
        return None


async def get_url_tile_async():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã"""
    url = 'https://www.keramogranit.ru/catalog-products/keramicheskaya-plitka/'

    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    q = requests.get(url=url, headers=headers)
    result = q.content
    soup = BeautifulSoup(result, 'lxml')

    pages_counts = int(soup.find_all('a', class_='pager__link')[-1].text)
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–±–æ—Ä–∞: {pages_counts}")

    url_list = []

    # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é
    async with aiohttp.ClientSession() as session:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        tasks = []
        for i in range(1, pages_counts + 1):
            page_url = f'https://www.keramogranit.ru/catalog-products/keramicheskaya-plitka/?p={i}'
            tasks.append(fetch_page_async(session, page_url, i, pages_counts))

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)

        async def fetch_with_semaphore(task):
            async with semaphore:
                return await task

        results = await asyncio.gather(*[fetch_with_semaphore(task) for task in tasks])

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for html in results:
            if html:
                soup = BeautifulSoup(html, 'lxml')
                pages = soup.find_all('div', class_='cat-card__desc')
                for page in pages:
                    try:
                        page_url = "https://www.keramogranit.ru" + page.find('a', class_='cat-card__title-link').get('href')
                        if '–º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤' not in page.find('div', class_='cat-card__price').text.strip():
                            url_list.append(page_url)
                    except:
                        pass

    url_list = list(set(url_list))
    url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_KeramogranitRu.txt')
    with open(url_file_path, 'w', encoding='utf-8') as file:
        for line in url_list:
            file.write(f'{line}\n')

    print(f"‚úì –°–æ–±—Ä–∞–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(url_list)}")


def get_url_tile():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
    asyncio.run(get_url_tile_async())


async def process_product_async(session, url, data_dict, break_line, lock, file_path, idx, total):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞"""
    try:
        html = await fetch_page_async(session, url)
        if not html:
            async with lock:
                break_line.append(url)
            return

        soup = BeautifulSoup(html, 'lxml')
        cur_data = datetime.now().strftime("%d.%m.%Y")
        cur_time = datetime.now().strftime("%H:%M")

        try:
            name = soup.find("div", class_='page-title').text.strip()
        except:
            name = "None"

        try:
            new_price = soup.find("span", class_='cat-price__cur').text.replace(' ', '').strip()
        except:
            new_price = 'Error'

        try:
            old_price = soup.find('del', class_='cat-price__del').text.replace(' ', '').strip()
        except:
            old_price = new_price

        try:
            price_units = soup.find("span", class_='cat-price__measure').text.strip()
        except:
            price_units = 'Error'

        try:
            stocs = soup.find('span', class_='cat-availibility__in').text
        except:
            stocs = None

        left_spec = []
        right_spec = []

        specs = soup.find('div', class_='cat-article-params').find_all('dt')
        for spec in specs:
            left_spec.append(spec.text.strip())

        rspecs = soup.find('div', class_='cat-article-params').find_all('dd')
        for spec in rspecs:
            right_spec.append(spec.text.strip())

        specs_dict = {left_spec[i].strip(): right_spec[i].strip() for i in range(len(left_spec))}

        data = {
            "–ü–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
            "–î–µ–π—Å—Ç–≤—É—é—â–∞—è —Ü–µ–Ω–∞": new_price,
            '–¶–µ–Ω–∞ –±–µ–∑ —Å–∫–∏–¥–∫–∏': old_price,
            "–ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è —Ü–µ–Ω—ã": price_units,
            '–í –Ω–∞–ª–∏—á–∏–∏': stocs,
            "–°—Å—ã–ª–∫–∞": url,
            "–î–∞—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_data,
            "–í—Ä–µ–º—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞": cur_time,
            "–ú–∞–≥–∞–∑–∏–Ω": "Keramogranit_ru",
        }

        # –ü–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        async with lock:
            data_dict.append(data | specs_dict)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 50 –∫–∞—Ä—Ç–æ—á–µ–∫
            if len(data_dict) % 50 == 0:
                save_data_incrementally(data_dict, file_path)

            # –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ 1000 –∑–∞–ø–∏—Å–µ–π
            if len(data_dict) % 1000 == 0:
                save_backup_copy(data_dict, file_path)

            print(f'‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx}/{total} | –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')

    except Exception as e:
        async with lock:
            break_line.append(url)
        print(f'‚úó –û—à–∏–±–∫–∞ ({idx}/{total}): {str(e)[:50]}')


async def get_data_async():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤"""
    print("\n" + "="*60)
    print("–û–ë–†–ê–ë–û–¢–ö–ê –¢–û–í–ê–†–û–í")
    print("="*60)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    data_dict = load_existing_data()
    processed_urls = get_processed_urls(data_dict)

    # 2. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ URL
    url_file_path = os.path.join(SCRIPT_DIR, f'url_list_{cur_data_file}_KeramogranitRu.txt')

    if not os.path.exists(url_file_path):
        print(f"‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {url_file_path}")
        return

    with open(url_file_path, 'r', encoding='utf-8') as file:
        all_lines = [line.strip() for line in file.readlines()]

    # 3. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    lines = [line for line in all_lines if line not in processed_urls]

    print(f"–í—Å–µ–≥–æ URL –≤ —Ñ–∞–π–ª–µ: {len(all_lines)}")
    print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_urls)}")
    print(f"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(lines)}")
    print("="*60 + "\n")

    if not lines:
        print("‚úì –í—Å–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
        return

    break_line = []
    file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_KeramogranitRu.json")
    total_urls = len(lines)

    # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é –∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    lock = asyncio.Lock()

    async with aiohttp.ClientSession() as session:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤
        tasks = []
        for idx, url in enumerate(lines, 1):
            tasks.append(process_product_async(session, url, data_dict, break_line, lock, file_path, idx, total_urls))

        # –í—ã–ø–æ–ª–Ω—è–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)

        async def process_with_semaphore(task):
            async with semaphore:
                return await task

        await asyncio.gather(*[process_with_semaphore(task) for task in tasks])

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f'\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–æ–≤—ã—Ö: {len(lines) - len(break_line)}')
    print(f'‚úó –û—à–∏–±–æ–∫: {len(break_line)}')
    print(f'‚úì –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(data_dict)}')

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è
    if len(data_dict) > 0:
        save_backup_copy(data_dict, file_path)
        save_data_incrementally(data_dict, file_path)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–º–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    if break_line:
        save_broken_urls(break_line)


def get_data():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
    asyncio.run(get_data_async())


async def retry_broken_urls_async():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫"""
    print("\n" + "="*60)
    print("–ü–û–í–¢–û–†–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –°–õ–û–ú–ê–ù–ù–´–• –°–°–´–õ–û–ö")
    print("="*60)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    data_dict = load_existing_data()
    processed_urls = get_processed_urls(data_dict)

    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞ —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏
    broken_urls_file = os.path.join(SCRIPT_DIR, f'url_break_list_{cur_data_file}_KeramogranitRu.txt')

    if not os.path.exists(broken_urls_file):
        print(f"‚úì –§–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return

    # 3. –ß–∏—Ç–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL
    with open(broken_urls_file, 'r', encoding='utf-8') as file:
        all_broken_urls = [line.strip() for line in file.readlines() if line.strip()]

    # 4. –§–∏–ª—å—Ç—Ä—É–µ–º - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    lines = [line for line in all_broken_urls if line not in processed_urls]

    print(f"–í—Å–µ–≥–æ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö URL: {len(all_broken_urls)}")
    print(f"–£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–Ω–µ–µ: {len(all_broken_urls) - len(lines)}")
    print(f"–ö –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(lines)}")
    print("="*60 + "\n")

    if not lines:
        print("‚úì –í—Å–µ —Å–ª–æ–º–∞–Ω–Ω—ã–µ URL —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
        return

    break_line = []
    file_path = os.path.join(SCRIPT_DIR, f"data_{cur_data_file}_KeramogranitRu.json")
    total_urls = len(lines)

    # –°–æ–∑–¥–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Å–µ—Å—Å–∏—é —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
    lock = asyncio.Lock()

    async with aiohttp.ClientSession() as session:
        tasks = []
        for idx, url in enumerate(lines, 1):
            tasks.append(process_product_async(session, url, data_dict, break_line, lock, file_path, idx, total_urls))

        # –ú–µ–Ω—å—à–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
        semaphore = asyncio.Semaphore(10)

        async def process_with_semaphore(task):
            async with semaphore:
                await asyncio.sleep(0.5)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                return await task

        await asyncio.gather(*[process_with_semaphore(task) for task in tasks])

    print(f'\n{"="*60}')
    print(f'‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(lines) - len(break_line)}')
    print(f'‚úó –í—Å—ë –µ—â—ë —Å–ª–æ–º–∞–Ω–æ: {len(break_line)}')
    print(f'‚úì –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {len(data_dict)}')
    print("="*60)

    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if len(data_dict) > 0:
        save_backup_copy(data_dict, file_path)
        save_data_incrementally(data_dict, file_path)

    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    if break_line:
        save_broken_urls(break_line)
        print(f"\n‚úì –û–±–Ω–æ–≤–ª—ë–Ω —Å–ø–∏—Å–æ–∫ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(break_line)} —à—Ç.")
    else:
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª —Å–æ —Å–ª–æ–º–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏, –µ—Å–ª–∏ –≤—Å–µ —É—Å–ø–µ—à–Ω–æ
        try:
            if os.path.exists(broken_urls_file):
                os.remove(broken_urls_file)
                print(f"\n‚úì –í—Å–µ —Å—Å—ã–ª–∫–∏ —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã! –§–∞–π–ª —É–¥–∞–ª—ë–Ω.")
        except:
            pass


def get_data_break():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
    asyncio.run(retry_broken_urls_async())


def main():
    print("="*60)
    print("–ü–ê–†–°–ï–† KERAMOGRANIT.RU –° –ê–°–ò–ù–•–†–û–ù–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–û–ô")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {CONCURRENT_REQUESTS}")
    print("="*60 + "\n")

    # 1. –°–±–æ—Ä —Å—Å—ã–ª–æ–∫ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞
    get_url_tile()

    # 2. –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
    get_data()

    # 3. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    retry_question = input('\n–í—ã –∂–µ–ª–∞–µ—Ç–µ –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É —Å–ª–æ–º–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫? ("1" - –î–∞; "0" - –ù–µ—Ç): ')
    if retry_question == "1":
        get_data_break()


if __name__ == '__main__':
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å aiohttp:
    # pip install aiohttp

    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö† –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    finally:
        finish_time = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"–ó–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —Ä–∞–±–æ—Ç—É —Å–∫—Ä–∏–ø—Ç–∞ –≤—Ä–µ–º—è: {round(finish_time, 2)} —Å–µ–∫—É–Ω–¥ ({round(finish_time/60, 2)} –º–∏–Ω—É—Ç)")
        print("="*60)
